# ğŸ¤– Inverse Pendulum Control using Reinforcement Learning & Classical Control

This project explores balancing an inverted pendulum (CartPole) using three control strategies:

- **PPO Reinforcement Learning (Stable-Baselines3)**
- **PID Controller**
- **LQR-like State Feedback Controller**

It compares results from AI-based control versus traditional control theory.

---

## ğŸ¯ Project Goal

- Keep the pole upright (\(\theta \approx 0\))
- Maintain the cart near the track center
- Optimize stability and handle disturbances

Applications in real-world systems include:
- Humanoid robot balance
- Segway stabilization
- Robotic manipulators

---

## ğŸ“Œ Problem Definition

### System: *Inverted Pendulum on a Cart*

State variables:
- `x` (cart position)
- `x_dot` (cart velocity)
- `theta` (pendulum angle in radians)
- `theta_dot` (pendulum angular velocity)

### Control Input:
Discrete force applied on the cart:
- `0` â†’ Push Left
- `1` â†’ Push Right

---

## ğŸš€ Solution Approaches

| Controller | Type             | Advantages               | Limitations                |
|------------|------------------|--------------------------|----------------------------|
| PID        | Feedback Control | Simple, quick setup      | Hard to tune, fails under large disturbances |
| LQR        | Optimal Control  | Good stability           | Requires full system model |
| PPO        | Deep RL          | Learns/adapts, robust    | Needs training time, less predictable        |

---

## ğŸ§  Reinforcement Learning (PPO)

PPO (Proximal Policy Optimization) trains a neural network to map observations to actions to maximize the time the pole stays balanced.

Training loop:
- Observe system state
- Decide push direction
- Get reward for balance
- Improve behavior iteratively

Saved model: `models/pendulum_ppo.zip`

---

## ğŸ”§ Classical Controllers

### PID
Uses error: `target_angle (0) â€“ theta`  
Control law:

$$u = K_p \cdot \text{error} + K_i \int{\text{error}\,dt} + K_d \frac{d(\text{error})}{dt}$$

Simple controller; works for small deviations, struggles with large disturbances.

### LQR-like Controller
State-feedback law:

$$u = -Kx$$

Where `K` is tuned for the linearized system. Offers stability but needs correct modeling.

---

## ğŸ§ª How to Run

### 1. Setup (Windows)
```
.\setup.bat
```
Creates `venv/` and installs modules.

Activate:
```
.\venv\Scripts\activate
```

### 2. Train RL Model
```
python src/train_ppo.py
```

### 3. Watch Trained Agent
```
python src/evaluate_ppo.py
```

### 4. Use Classical Controllers
```
python src/classical_pid_control.py
```
```
python src/classical_lqr_control.py
```

---

## ğŸ“‚ Project Structure

```Text
InversePendulum-RL/
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ train_ppo.py           # Train PPO RL agent
â”‚ â”œâ”€â”€ evaluate_ppo.py        # Test trained model
â”‚ â”œâ”€â”€ classical_pid.py       # PID controller
â”‚ â””â”€â”€ classical_lqr.py       # LQR state feedback
â”œâ”€â”€ models/                  # Saved RL models (*.zip)
â”œâ”€â”€ results/                 # Plots, logs, videos
â”œâ”€â”€ requirements.txt         # Dependencies
â”œâ”€â”€ setup.bat                # Windows setup
â””â”€â”€ README.md
```
---

## ğŸ“Š Results

Control Method | Avg Balance Time | Notes
---------------|------------------|-----------------------------------
PID            | â­â­â˜†â˜†â˜†        | Works only for small angles
LQR            | â­â­â­â­â˜†      | Stable & quick response
PPO            | â­â­â­â­â­     | Best after training; most robust

RL achieves the most robust balancing under disturbances.

---

## ğŸ“ˆ Future Improvements

- Add noisy sensors (realistic conditions)
- Use continuous action space (SAC / TD3)
- Add result graphs to README
- Export controller to Arduino robot

---

## ğŸ§‘â€ğŸ’» Technologies

- Python 3.10
- Gymnasium
- Stable-Baselines3
- PyTorch
- NumPy

---

## ğŸ† Credits

Project by: Saumya Shah & Astha  
For learning and research in control systems + AI.

---

â­ If you like this project, star the repo!

---
